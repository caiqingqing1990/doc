途牛调度中心（简称LLT）。LLT 为途牛提供统一的定时脚本调度服务。该服务主要侧重于脚本的调度、脚本执行监控、预警、脚本调度依赖管理、脚本的日志管理等。特点是同步配置多个IP+PORT避免脚本执行的单点问题。
功能说明：

   任务注册：根据应用系统需求设定任务类型(no-state/state）、可并发或不可并发，调度类型：接口任务、脚本任务，执行任务类型单次/定时。
 
   任务&计划管理：提供任务执行、启动调度、终止调度、注销、新增、修改、删除；计划新增、修改、删除。
 
   任务监控：提供任务执行状态、任务执行日志、任务超时，任务失败邮件提醒等功能
 
   任务串联管理：提供子任务、及子任务串联规则。
 
   用户权限管理：提供用户组、成员权限管理
   
   优点：

   可集群部署，避免单点故障
 
   同一时刻调度可并发可串行
 
   可以监控的执行情况
 
   可以灵活管理单个脚本发布等
 
   有效管理脚本依赖关系
 
   统一的日志管理
 
   统一配置管理
 
   调度失败重试机制

支持调度方式：

    接口方式

         1、JAVA BEAN

    脚本方式：

         1、SHELL

         2、PHP

         3、PHP YII 框架

任务串联设计（暂不支持）

5394 个任务  一天执行  2,562,352 个

TSP提供对当前服务的治理方案，主要作用侧重于服务治理，所有的服务以及服务间的调用关系将在TSP中得到治理与监控，其特点是对当前系统影响较小，改造成本较低。
TSP主要功能包含但不限于应用服务的注册审核管理、应用服务运行状态的监管、应用服务负载均衡、应用服务资源治理。

角色说明：

l  注册中心：服务注册、审核、变更、发现。

l  服务提供者：暴露服务接口的服务提供方。

l  服务消费者：调用远程服务接口的服务消费方。

l  监控中心：统计服务的调用次数、时间、结果等。

调用关系说明：

l  服务提供者启动时，向注册中心注册自己提供的服务接口。

l  服务消费者启动时，向注册中心订阅自己需要的服务接口。

l  注册中心返回服务提供者的地址列表给消费者，如果有变更，注册中心将推送变更后的新地址列表给消费者。

l  服务消费者，从提供者地址列表中，基于设置的负载均衡策略、地域策略等选择一台提供者进行服务调用，如果失败，根据失败策略，重新选择其他提供者。

l  服务消费者和提供者，实时统计服务调用/消费次数、时间、结果，定时发送数据到监控中心。

l  监控中心负责收集监控数据，并进行合理分析，提供日后的系统维护升级的依据。

 

连通关系说明：

l  注册中心不负责转发服务请求，实际的服务请求由服务提供者和服务消费者之间直接连通。

l  注册中心和服务提供者之间、注册中心和服务消费者之间均使用长连接保持心跳，互相感知保活状态。

l  注册中心和服务消费者之间使用长连接保持心跳互相感知保活状态。当有服务地址的变更时，注册中心复用该长连接进行变更通知。

l  注册中心主从部署，主从之间长连接感知活动状态。主宕机，从接替，并重新建立与所有服务提供者、服务消费者的长连接。

 

可用性说明：

l  监控中心宕机不影响系统运行。

l  注册中心主从部署，主宕机，从接替。

l  注册中心整体宕机，则服务地址列表更新，服务审核、服务上线等功能将不可用。但是服务消费者可以根据本地的缓存地址列表进行正常服务调用。在当前服务提供者不出现宕机的情况下，系统将可以正常运行，但新加入服务提供者将不会被感知。

l  服务提供者一台宕机，不影响系统运行。

l  服务提供者全部宕机，服务消费者的服务不可达，根据设置的重试策略进行重试及失败。

l  服务提供者可以动态增加部署的实例。

l  新服务提供者启动，如果服务已经通过审核则可直接使用，未审核服务将待审核流程完成才能通过系统被服务消费者感知。

l  子系统全部切换后，不再需要Haproxy/Nginx等负载均衡服务器，避免负载均衡服务器宕机导致子系统不可用。

大部分服务不再依赖DNS服务，减少DNS解析，并有效屏蔽DNS的不稳定。

TSP 日志优化
MQ-FLUME-KAFKA-MONITOR-ES

2W QPS

文件系统2.0设计方案
前言

目前公司使用的文件系统是filebroker，filebroker的存储依赖商业的共享存储方案NetApp。成本高昂，扩容流程复杂。使用共享存储的应用较多，相互之间有影响。随着业务增加，公司社会化营销的需要，filebroker使用量逐日增加，突出的问题如下：

存储不足，不能灵活扩容，成本较高，2W/T；
使用共享存储的应用较多，相互之间有影响；
NetApp本身是高可用的，但是如果NetApp集群不可达则导致filebroker直接不可用；
filebroker的文件备份完全依赖NetApp，无法自行控制；
filebroker的路径规划分三种：子系统+日期、子系统+md5、子系统+固定路径，小文件众多的情况下会导致一个目录下文件太多，影响效率。
filebroker应用本身存储逻辑和文件转换逻辑在一个应用上，个别转换业务出问题可能影响基本的上传下载；
filebroker的文件转换功能依赖各种组件和字体，部署成本高；
针对以上问题，底层组着手开发基于分布式文件系统的filebroker 2.0


性能
 下载接口TPS 4000 50K文件大小；
上传接口TPS 2000 50K文件大小；
删除接口TPS 5000 50K文件大小；
最高要求：忽略CDN的情况下，可以直接抗住下载请求，对缓存服务器要求高；
容错
 单个filebroker文件计算web服务实例宕机不影响服务；
单个filebroker文件存储web服务实例宕机不影响服务；
单个Varnish缓存服务实例宕机不影响服务；
FastDFS-tracker单台宕机不影响服务；
FastDFS-storage单台宕机不影响服务；
FastDFS-storage单台服务上storage实例挂掉不影响服务；
FastDFS-storage单台服务上nginx实例挂掉不影响服务；
安全控制
 外网可访问的文件夹在可控范围内；外网不能通过文件夹操作来获取可访问路径外的文件；如禁止../../，*这样的操作；
合同等文件外网不能随意访问，需要前台网站做跳转，访问控制需要严格控制；
文件删除接口有严格的控制，系统初期不对外开发，只做内部维护使用，系统提jira请求，维护人员通过系统批量删除；
详细设计

硬件容错
 任一节点服务器不可用，不会影响整体服务，因为每个节点都是集群，不存在单点；
任一节点的机架不可用，不影响整体服务，各个节点部署在不同机架上，不存在机架单点；
任一节点的交换机不可用，不影响整体整体服务，要求交换机有热备；这里需要给出系统的网络拓扑，分析出风险点，至少保证部分文件可达，服务有效；
故障机器移除：故障节点直接关闭storaged进程和nginx服务；
监控及告警
 FastDFS集群采用独立的监控系统，了解集群状态，实时监控tracker节点状态，storage各节点的live状况，storage磁盘使用情况预警。
Varnish缓存集群，需要监控各个实例的live状态，关注内存使用，磁盘io，网络usage；
使用通用的zabbix监控filebroker存储服务各个实例的上传、删除接口服务状态；
使用通过的zabbix监控filebroker计算服务各个实例上的各http接口可用状态；
使用通过的zabbix监控filebroker计算服务各服务器上转换进程数量；
使用通过的zabbix监控filebroker计算服务各实例的jvm状态：线程数，堆内存

前言

1、为避免因外网CDN回源，导致请求直接穿透到后端nginx带来的系统高负载的风险。

2、切图的图片是以fastdfs从文件的方式存储，浪费storage存储空间。

3、新文件系统的图片现只有外网CDN的缓存，需要对新文件系统的文件进行缓存。

针对以上问题，优化系统，采用varnish 作为缓存。

系统概述
为了避免对线上原有系统的部署进行大量的变动，现操作为在原有的 LVS+NGINX 或者NetScaler 和 NGINX+lua 之间 增加一个Nginx集群和Varnish 集群。

增加的nginx 集群，我们称为nginx+lua+hash，主要做url的拆分转发，上传、计算的逻辑转发到后端的NGINX+lua模块，文件下载则通过hash打到Varnish集群中的某台机器。

Varnish集群主要做文件缓存处理，

设计说明：

1、外网访问m.tuniucdn.com 由过Netscaler 采用一致性hash策略 打到后端的varnish集群，由varnish访问Nginx，并对文件做缓存，内网访问不变。

2、外网的流量以切图请求为主，切图生成的图片不再存储到storage，而是由缓存到varnish。

3、单独部署切图应用的集群，来处理切图的请求，打到实时切图的性能要求。

 

统计：

每天外网图片访问量：19,421,885 条 高峰每秒大概400 条

每天切图量：328,743  每秒大概需要切 7条

当后端图片能够达到进行实时切图，（不考虑前端varnish缓存情况，只考虑最差情况）后端需要达到支撑每秒400条的切图压力，按照后端85次/秒计算，后端单独切图应用要达到5台。

带密码访问文件服务设计
一些客人的隐私文件需要访问又不想走登录流程，则提供一个带密码访问的文件服务。提供一个短地址和对应的密码，客人访问短地址后，需要输入密码才能访问包装的文件。

一天 796,206 上传量 一天  上传耗时 1M 大概22 mm 一秒钟20个 


文件系统平台化
17.05.07 启动

 

2017.05.12 完成梳理相关功能点，概要设计，功能内容



2017.05.25 完成sdk client开发 v1.0.0



2017.06.05 完成快速构建fdfs脚本

 

2017.06.15 存储服务功能点改造

 

2017.06.21 计算服务功能点改造



2017.07.16 全流程自测 ，manager开发，web-console 接入pebble控制台，

提供管理操作完成

 

2017.07.21 使用说明文档

fb2 对外服务抽象接口（包括，计算 和 存储）	client	高	100%
client 加入access认证，签名	client	高	100%
client 支持http连接池	
client

高	100%
支持文件上传大小和个数限制	client	高	100%
快速构建fdfs脚本，FDFS 快速搭建	fdfs-build	低	100%
存储加入接入认证，分组识别	fbs-store	高	100%
计算支持 认证透传 ，校验，依赖client jar 进行文件上传操作	fb2-calc	高	100%
web支持存储申请，分配，启用禁用等操作	manager	高	100%
manager 开发、支持存储监控，分组管理，异常告警	manager	高	100%
使用说明文档 FB文件平台对接	doc	中	0%
使用sdk性能测试	doc	中	0%
v2	 
后续 v2 release

支持 fdfs 一键部署（nginx 转发 动态修改，新增）

支持token，无需每次都认证加密

添加traceid 记录从计算服务到存储服务路径

丰富manager功能，实时监控上传文件个数，流量等

负载均衡策略实时计算

自动化构建	fdfs-build	中	0%
客户端服务端token支持	web-console	中	0%
traceid 日志记录	manager	中	0%
监控丰富	manager	中	0%



分库算法
申请了16个库，每个库16张表，一共256张表。命名规则是pla_sus_url_DB_TB。例如第1个库第10张表表名为pla_sus_url_00_09
短址通过62进度转10进制，转换成一个long值key（详情参见下文）。
数据库序号=key%16
表序号=(key/dbSize)%tableSize

短址与数据库主键关系
短址与数据库主键ID(long)是可以互相转换的。该设计模式可以提升短址访问速度，短址访问时的数据库查询都是主键查询。具体算法如下
主键在数据库定义是的bigint，范围是0~2^62
短址是62进制，每一位的值可以是0-9,a-z,A-z，范围是0~62^N，n是短址位数，目前是6位，用完时再增加一位。
转换时就是62进制和10进制之间的相互转换。

短址生成
上文提到，短址其实就是数据库主键。因此生成短址，只需要生成唯一的数据库主键即可。由于数据库是分布式部署，一共16个库，256张表。并不能使用简单的auto_increment。
最终实现是通过发号器服务，由发号器统一分配自增的ID。线程先访问发号器拿到ID后再插入数据库。
发号器依赖redis的incr命令来实现自增，再异步写入数据库。redis不可用时，使用事务在数据库实现自增。

短址加密解密
系统内置了多个码表，防止遍历。在数据库连续的ID转换为最终的短址后是乱序的

http://tool.lu/hexconvert/


